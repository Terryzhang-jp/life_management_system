"use client"

import { useState, useEffect, useRef, useCallback } from 'react'
import {
  RecorderState,
  RecognitionProvider,
  UseSonioxRecorderOptions,
  UseSonioxRecorderReturn,
  SonioxPartialResult,
  SonioxToken
} from './types'

/**
 * Soniox è¯­éŸ³è¯†åˆ« React Hookï¼ˆæ­£ç¡®å®ç°ï¼‰
 *
 * åŸºäºå®˜æ–¹æ–‡æ¡£çš„æ­£ç¡®æ–¹å¼ï¼š
 * 1. åªä½¿ç”¨ onPartialResult å¤„ç†ç»“æœ
 * 2. ç§»é™¤ include_nonfinalï¼ˆå·²åºŸå¼ƒï¼‰å’Œ onResultï¼ˆä¸å­˜åœ¨ï¼‰
 * 3. ä½¿ç”¨ is_final åŒºåˆ†å†å²å’Œä¸´æ—¶ tokens
 * 4. å¯é€‰æ˜¾å¼ä¼ é€’ MediaStream
 */
export function useSonioxRecorder(options: UseSonioxRecorderOptions): UseSonioxRecorderReturn {
  const {
    apiKey,
    language = 'zh',
    onTranscriptUpdate,
    onError,
    enableFallback = true
  } = options

  // çŠ¶æ€
  const [state, setState] = useState<RecorderState>('idle')
  const [transcript, setTranscript] = useState('')
  const [interimTranscript, setInterimTranscript] = useState('')
  const [error, setError] = useState<string | null>(null)
  const [provider, setProvider] = useState<RecognitionProvider>('none')

  // Refs
  const sonioxClientRef = useRef<any>(null)
  const recognitionRef = useRef<any>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)

  // æ˜¯å¦æ­£åœ¨å½•éŸ³
  const isRecording = state === 'recording'

  // åˆå§‹åŒ– Sonioxï¼ˆæ­£ç¡®æ–¹å¼ï¼‰
  useEffect(() => {
    if (typeof window === 'undefined') return

    const initSoniox = async () => {
      try {
        const { SonioxClient } = await import('@soniox/speech-to-text-web')

        if (!apiKey) {
          throw new Error('SONIOX_API_KEY æœªé…ç½®')
        }

        // æ­£ç¡®çš„ SonioxClient åˆå§‹åŒ–
        const client = new SonioxClient({
          apiKey: apiKey,

          // âœ… åªåœ¨è¿™é‡Œè®¾ç½® onPartialResultï¼ˆå®˜æ–¹æ­£ç¡®æ–¹å¼ï¼‰
          onPartialResult: (result: SonioxPartialResult) => {
            console.log('ğŸ“ Partial result:', result)
            processTokens(result.tokens)
          },

          onError: (status: string, message: string) => {
            console.error('âŒ Soniox error:', status, message)
            const errorMsg = `è¯­éŸ³è¯†åˆ«é”™è¯¯: ${message}`
            setError(errorMsg)
            setState('error')
            onError?.(new Error(errorMsg))

            // Fallback
            if (enableFallback && provider === 'soniox') {
              console.log('ğŸ”„ Soniox å¤±è´¥ï¼Œåˆ‡æ¢åˆ° Web Speech API')
              fallbackToWebSpeech()
            }
          },

          onStateChange: (state: string) => {
            console.log('ğŸ”„ Soniox state change:', state)
          }
        })

        sonioxClientRef.current = client
        setProvider('soniox')
        console.log('âœ… Soniox å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ')
      } catch (err: any) {
        console.error('âŒ Soniox åˆå§‹åŒ–å¤±è´¥:', err)
        setError('Soniox åˆå§‹åŒ–å¤±è´¥')

        // å¯ç”¨ fallback
        if (enableFallback) {
          console.log('ğŸ”„ å°è¯•ä½¿ç”¨ Web Speech API')
          initWebSpeech()
        } else {
          setState('error')
        }
      }
    }

    initSoniox()

    return () => {
      // æ¸…ç†èµ„æº
      if (sonioxClientRef.current) {
        try {
          sonioxClientRef.current.stop()
        } catch (e) {
          console.warn('æ¸…ç† Soniox å®¢æˆ·ç«¯æ—¶å‡ºé”™:', e)
        }
      }
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop()
        } catch (e) {
          console.warn('æ¸…ç† Web Speech æ—¶å‡ºé”™:', e)
        }
      }
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop())
      }
    }
  }, [apiKey, enableFallback])

  // å¤„ç† tokensï¼ˆæ­£ç¡®æ–¹å¼ï¼šæŒ‰ is_final åŒºåˆ†ï¼‰
  const processTokens = useCallback((tokens: SonioxToken[]) => {
    if (!tokens || tokens.length === 0) return

    const finalTokens: string[] = []
    let interimText = ''

    for (const token of tokens) {
      const text = token.text || ''

      // âœ… åªè¿‡æ»¤ <fin> æ ‡è®°ï¼Œä¸è¿‡æ»¤æ‰€æœ‰ <>
      if (text === '<fin>') {
        console.log('âœ… æ”¶åˆ° finalization æ ‡è®°')
        continue
      }

      // âœ… æŒ‰ is_final åŒºåˆ†å†å²å’Œä¸´æ—¶
      if (token.is_final) {
        finalTokens.push(text)
      } else {
        interimText += text
      }
    }

    // æ›´æ–°å†å² transcriptï¼ˆç´¯åŠ  final tokensï¼‰
    if (finalTokens.length > 0) {
      const finalText = finalTokens.join('')
      console.log('âœ… Final tokens:', finalText)
      setTranscript(prev => prev + finalText)
    }

    // æ›´æ–°ä¸´æ—¶ transcriptï¼ˆæ›¿æ¢ï¼Œä¸ç´¯åŠ ï¼‰
    console.log('ğŸ’­ Interim text:', interimText)
    setInterimTranscript(interimText)

    // è§¦å‘å›è°ƒ
    onTranscriptUpdate?.(transcript, interimText)
  }, [transcript, onTranscriptUpdate])

  // åˆå§‹åŒ– Web Speech API (fallback)
  const initWebSpeech = useCallback(() => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      setError('æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«')
      setState('error')
      setProvider('none')
      return
    }

    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
    const recognition = new SpeechRecognition()

    recognition.continuous = true
    recognition.interimResults = true
    recognition.lang = language === 'zh' ? 'zh-CN' : 'en-US'

    recognition.onresult = (event: any) => {
      let finalText = ''
      let interimText = ''

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript
        if (event.results[i].isFinal) {
          finalText += transcript
        } else {
          interimText += transcript
        }
      }

      if (finalText) {
        setTranscript(prev => prev + finalText)
      }
      setInterimTranscript(interimText)

      onTranscriptUpdate?.(transcript, interimText)
    }

    recognition.onerror = (event: any) => {
      console.error('âŒ Web Speech API é”™è¯¯:', event.error)
      setError(`è¯­éŸ³è¯†åˆ«é”™è¯¯: ${event.error}`)
      setState('error')
    }

    recognitionRef.current = recognition
    setProvider('webspeech')
    console.log('âœ… Web Speech API åˆå§‹åŒ–æˆåŠŸ')
  }, [language, onTranscriptUpdate])

  // Fallback åˆ° Web Speech API
  const fallbackToWebSpeech = useCallback(() => {
    if (provider === 'webspeech' || !enableFallback) return
    initWebSpeech()
  }, [provider, enableFallback, initWebSpeech])

  // å¼€å§‹å½•éŸ³ï¼ˆæ­£ç¡®æ–¹å¼ï¼‰
  const startRecording = useCallback(async () => {
    try {
      setError(null)
      setInterimTranscript('')
      setState('recording')

      if (provider === 'soniox' && sonioxClientRef.current) {
        // âœ… ç®€åŒ–é…ç½®ï¼šè®© SDK è‡ªå·±å¤„ç†éº¦å…‹é£ï¼ˆä¸æ‰‹åŠ¨ä¼  streamï¼‰
        console.log('ğŸ¤ å¼€å§‹ Soniox å½•éŸ³ï¼ˆSDK è‡ªåŠ¨é‡‡éº¦ï¼‰...')

        // âœ… æ­£ç¡®çš„ start é…ç½®ï¼ˆè®© SDK è‡ªå·±é‡‡éº¦ï¼‰
        await sonioxClientRef.current.start({
          model: 'stt-rt-preview',

          // âŒ ä¸ä¼  streamï¼Œè®© SDK è‡ªå·±å¤„ç†
          // stream: stream,

          // âœ… ä½¿ç”¨é©¼å³°å‘½å
          languageHints: [language],  // åªç”¨å•è¯­è¨€è¯•è¯•
          // enableLanguageIdentification: false,  // å…ˆç¦ç”¨è¯­è¨€è¯†åˆ«
          // enableEndpointDetection: false,  // å…ˆç¦ç”¨ç«¯ç‚¹æ£€æµ‹

          onStarted: () => {
            console.log('âœ… Soniox å½•éŸ³å¼€å§‹')
            setState('recording')
          },

          onFinished: () => {
            console.log('â¹ï¸ Soniox å½•éŸ³ç»“æŸ')
            setState('idle')
            setInterimTranscript('')
          }
        })

        console.log('âœ… Soniox start è°ƒç”¨æˆåŠŸ')
      } else if (provider === 'webspeech' && recognitionRef.current) {
        // ä½¿ç”¨ Web Speech API
        recognitionRef.current.start()
        console.log('ğŸ¤ Web Speech API å½•éŸ³å¼€å§‹')
      } else {
        throw new Error('è¯­éŸ³è¯†åˆ«æœªåˆå§‹åŒ–')
      }
    } catch (err: any) {
      console.error('âŒ å¼€å§‹å½•éŸ³å¤±è´¥:', err)
      setError(`å¼€å§‹å½•éŸ³å¤±è´¥: ${err.message}`)
      setState('error')
      onError?.(err)
    }
  }, [provider, language, onError])

  // åœæ­¢å½•éŸ³ï¼ˆæ­£ç¡®æ–¹å¼ï¼‰
  const stopRecording = useCallback(() => {
    try {
      if (provider === 'soniox' && sonioxClientRef.current) {
        // âœ… ä½¿ç”¨ stop() ä¼˜é›…ç­‰å¾…æ‰€æœ‰ final tokens
        sonioxClientRef.current.stop()
        console.log('â¹ï¸ åœæ­¢ Soniox å½•éŸ³')
      } else if (provider === 'webspeech' && recognitionRef.current) {
        recognitionRef.current.stop()
        console.log('â¹ï¸ åœæ­¢ Web Speech API å½•éŸ³')
      }

      // åœæ­¢éº¦å…‹é£
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop())
        mediaStreamRef.current = null
      }

      setState('idle')
      // æ¸…ç©ºä¸´æ—¶æ–‡æœ¬
      setInterimTranscript('')
    } catch (err: any) {
      console.error('âŒ åœæ­¢å½•éŸ³å¤±è´¥:', err)
    }
  }, [provider])

  // æ‰‹åŠ¨è§¦å‘ finalizationï¼ˆæ­£ç¡®æ–¹å¼ï¼‰
  const finalize = useCallback(() => {
    if (provider === 'soniox' && sonioxClientRef.current && isRecording) {
      try {
        // âœ… ä½¿ç”¨ finalize() ç«‹å³æŠŠ non-final å˜æˆ final
        sonioxClientRef.current.finalize()
        console.log('âœ… æ‰‹åŠ¨ finalization è§¦å‘')
      } catch (err) {
        console.error('âŒ Finalization å¤±è´¥:', err)
      }
    }
  }, [provider, isRecording])

  // æ¸…ç©ºè½¬å½•æ–‡æœ¬
  const clearTranscript = useCallback(() => {
    setTranscript('')
    setInterimTranscript('')
  }, [])

  // é‡ç½®é”™è¯¯
  const resetError = useCallback(() => {
    setError(null)
    if (state === 'error') {
      setState('idle')
    }
  }, [state])

  return {
    state,
    isRecording,
    transcript,
    interimTranscript,
    error,
    provider,
    startRecording,
    stopRecording,
    finalize,
    clearTranscript,
    resetError
  }
}
